{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO0+Bz2vcXuNItiZAdgHwr0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabrieldimas/machine-learning-study-2023/blob/main/Week9/week9_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Praktikum 2"
      ],
      "metadata": {
        "id": "1HQe0hvlzVhN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import TensorFlow"
      ],
      "metadata": {
        "id": "5mmWf3JPzqqo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LKWa9d7oyoYb"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Dataset Shakespeare"
      ],
      "metadata": {
        "id": "gulpi4r_zsPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5ROaH_dzXhg",
        "outputId": "0267a9d9-b970-49a2-f70d-09fc3998a1da"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 1s 1us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data"
      ],
      "metadata": {
        "id": "3gIspQvtzuBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLMI5SjKzhad",
        "outputId": "f1bfb909-3aad-42d9-85d8-f8c737eebc09"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNNbsJAkzld_",
        "outputId": "ab2f876d-bfde-43c4-f944-375e6ef91aef"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hktoMLhxznIk",
        "outputId": "5834560b-2091-4f9d-e4cf-1165d106ce1d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Olah Teks\n",
        "\n",
        "Vectorize Teks"
      ],
      "metadata": {
        "id": "Etu0YLkhzxEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTehOftFzoKb",
        "outputId": "abd1e7ff-685a-4da9-b568-bc68db5e541b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "wjavATrLzzxq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbX9qlGLz5MN",
        "outputId": "6928b7ab-df85-477b-ff85-acb1489c557d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "sRRQ6j7T0Gqk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRO3b8KC0J40",
        "outputId": "5251ec09-4aa0-4377-c557-b33856de32e3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vf61npNj0Nq1",
        "outputId": "b7e68e5c-6f53-4ba2-bab3-5aea46c51ca8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "mvogIFNz0RHC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Membuat Training Set dan Target"
      ],
      "metadata": {
        "id": "a7Wz-qqrCft7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "id": "FN0NrISn0c0q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be30c0b3-6e90-4a22-ae0e-6eea4c4a4a2a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "DscCWYGMCirW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EM8N_vFCnqV",
        "outputId": "d1046851-45d7-41ce-eb4c-3932a9ef992b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100"
      ],
      "metadata": {
        "id": "4VBhuqOqCq5E"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dodVi1TOCuA1",
        "outputId": "3b521138-2959-4453-e269-f624eff74684"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jC2_Vey0CwHg",
        "outputId": "14dfd924-ca18-43a0-a84d-56861222ab83"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        " input_text = sequence[:-1]\n",
        " target_text = sequence[1:]\n",
        " return input_text, target_text"
      ],
      "metadata": {
        "id": "Qh2GuXkkCyCn"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWb-gDp7C6F4",
        "outputId": "6b415617-4554-40b4-acb8-de21b62aaa93"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "iTfutZIRC8ob"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        " print(\"Input :\", text_from_ids(input_example).numpy())\n",
        " print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xJnEr-uC-g5",
        "outputId": "aa746107-eddd-4a1c-e4d1-139cf4936664"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oyCYVITDEPQ",
        "outputId": "5a7c993b-1182-4ba1-869c-583010684511"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Buat Model"
      ],
      "metadata": {
        "id": "R6iyrnG0DI05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "NzoILEJFDGe-"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "0teNtOJ-DKlC"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "VB5AzS9cDMex"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uji Model"
      ],
      "metadata": {
        "id": "AkcwvlqxDQ8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJAhcwpZDO5j",
        "outputId": "ee4ff73b-bf59-451e-cbab-b583298867c6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tkpS-seDTJL",
        "outputId": "bb4fb92f-4240-4189-9922-b2ac1dad98ca"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "ANRKWWbmDXAd"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WlD1r7sDbzH",
        "outputId": "391d884c-b36a-4c1f-afb6-d5ab114c632e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([25, 37, 19, 19, 38, 20, 44, 55, 36, 45, 21, 40, 61, 44, 49, 29, 43,\n",
              "       63, 50, 65, 23, 24, 64, 10,  8, 19, 34, 56, 60, 23, 42,  0, 65, 40,\n",
              "       31, 55, 49, 50, 59, 61, 33,  0, 22,  8, 47, 49, 21, 60,  8, 56, 13,\n",
              "       56,  5, 29, 28, 18,  6, 23, 44, 17, 21, 48,  8, 23,  6, 54, 59, 32,\n",
              "       61, 50, 21, 43, 33, 52, 33, 59, 39, 34, 64, 19, 13, 50, 60, 46, 22,\n",
              "        1, 46, 34, 55, 56, 13,  5, 38, 50, 40, 60,  9, 57, 33,  1])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9q3kS_DDeq_",
        "outputId": "2063d26c-7ed2-47a2-efe6-b4ce2ea36d62"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b\"their hats.\\nBut come, my lord, let us away.\\n\\nHASTINGS:\\nGo on before; I'll talk with this good fellow\"\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"LXFFYGepWfHavejPdxkzJKy3-FUquJc[UNK]zaRpjktvT[UNK]I-hjHu-q?q&POE'JeDHi-J'otSvkHdTmTtZUyF?kugI\\ngUpq?&Ykau.rT\\n\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Model"
      ],
      "metadata": {
        "id": "P18BNa3bDkxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "r_17NOZTDiGj"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqRw4NpODm9Y",
        "outputId": "b53747a9-23bc-4fb9-8775-03adf10e470d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.189595, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ER9-aEzNDoqx",
        "outputId": "39235d22-856f-4017-f670-59e043f80f30"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65.99607"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "Roi45CPmDqDL"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Konfigurasi Checkpoints"
      ],
      "metadata": {
        "id": "jiNrTzotDt8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "_pLXCmYdDr-U"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "P2VE8DqHFJhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ci7WnROwDvwN",
        "outputId": "93d0a577-eb53-4e92-89ff-1e50907016a0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 14s 53ms/step - loss: 2.7165\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 10s 51ms/step - loss: 1.9953\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 10s 51ms/step - loss: 1.7150\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 10s 52ms/step - loss: 1.5497\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 1.4480\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 12s 52ms/step - loss: 1.3810\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 1.3280\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 1.2820\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 1.2408\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 1.2011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "96zHR_FLFMM_"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "Ki3-TO66FMwB"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKUWdKkCFOi-",
        "outputId": "07c1c350-034b-4340-9176-0171b8714a2e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "I wart they. Faults, his round I take thyself:\n",
            "He hath since thou sweathest?\n",
            "\n",
            "Narse:\n",
            "To as he take all a wency,\n",
            "No willetion, the keepest offer of death,\n",
            "But see no little; but get thee shakeweth a boot\n",
            "Afort Warwick nothing endurier.\n",
            "My ghist I will not parfold true presuge\n",
            "Shall ence her eirs a stranger in the Triou in death--\n",
            "He did ne'er all unboats hither Montague;\n",
            "Though the widow in the sleep on thee,\n",
            "Thrill-gentle most courage, Eirs: do not tell thee.\n",
            "He cives me promised frost.\n",
            "\n",
            "CORIOLANUS:\n",
            "And I cattly; away!\n",
            "\n",
            "BUCKINGHAM:\n",
            "My grace to you! tell the heavy end, who let is stands repose:\n",
            "And slain, and such a tame-land after.\n",
            "\n",
            "ESCALUS:\n",
            "I have sent from everitan; can you calone\n",
            "With those that's to the triumph'd men;\n",
            "And men are all enough; and wilt thou speak to braw.\n",
            "saw ye, if we burned virtuous?\n",
            "\n",
            "AUFIDIUS:\n",
            "Lead to him; I saw not sweet As; forgot,\n",
            "To save him good myself.\n",
            "How now! where the next news shall never see\n",
            "Where the daughter husband's mercy and labour\n",
            "Of this free wr \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.69930100440979\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "515WuYigFTBh",
        "outputId": "8e7fdad5-f66c-456e-edfa-a26066cd5f03"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nHer after night to your good worshilate,\\nMe deceit stir as the very tricks\\nAnd purge be subslius: and smilennesh beggars:\\nMecher's the mide Of thy heapt loved thee how\\nCannot my heart of the cape but thyself\\nAnd his paple--made the feathen's foom, and sing of sign\\nTo hear a surman to her pleasants! stay, grag now!\\nJuliet issue of dicled! if he'r sleep! leave the\\ntook to his thrown three quest.\\n\\nADWh:\\nNelly none benind that my honour, gentle man,\\nI may not know of adrian fear'd.\\n\\nMENENIUS:\\nHow dreys your heart hath my too cold?\\nWhy then,\\nAs he has enter'd up as tells himself.\\n\\nLORDS:\\nHard you so? Let's sorrow:\\nO whether of what? I speak not I am angry,\\nThe prention and a mopilia that rather hence,\\nHis hand of Lord of Wastictst tongue give\\nKing of his that dod soldiers' dow\\nRepuadments levied to your honour:\\nI then to surmon our hands, gentle heart\\nour meriting here inconscare to jan\\nThat thou laised out in imposits to answer,\\nTroubled by the sensely have been as us;\\nRed up your brother\"\n",
            " b\"ROMEO:\\nIf that came upon his great Apparent?\\n\\nKING RICHARD III:\\nCame, Grey or flatt? how it needs bet,\\nAnd lief beholding these and by broady\\nApen in the Gloucesters of Warwick, wherefore go thit\\nenough the Duke of York, for that, senden;\\nNear without me.\\n\\nProvost:\\nI will grant me murder.\\n\\nCORIOLANUS:\\nYour wife! let me gorb; you have tred to-morrow\\nProve intency!\\nLove good night owerport of young Paulina,\\nEven too long by the brobless\\nkings.\\n\\nBRAKENBURY:\\nI thank your wersake is I:\\nWith dear predicion to success.\\n\\nSIRGSLUCHIO:\\nThe goddles of a word in As hants.\\n\\nFirst Keeper:\\nFrom fool! they quaint-like two, her husband's kin!\\nAdvance thy boy, the ears hide for hered virtuous\\nEschange; most wrong'd, though thou betray'd?'\\nAnd yet, I prisoner, and Proud Bolingbroke's;\\nHolk villia, Sir, was Coriolanus;' will'd at my trues.\\n\\nISABELLA:\\nI pray--\\nTo mercy his quaintain'd frown, as to the Capulet;\\nFor 'tis a more refuse of signivy\\nWho epragh of hope, and to, thy surpier\\nAndwal voices; that I may be \"\n",
            " b\"ROMEO:\\nThere midder; my white I;\\nWhy dream all thus I. Take he sword?\\n\\nKING RICHARD III:\\nTwo of the hollow bried! O, speak of what;\\nFor I have discreentled's little, now too,\\nIf not marry her A wretch it,\\nOne thousand father's land! Marcy to your enemy,\\nAnd know obey'd borsh protectors.\\nMy lord, the crown have proved to strange him.\\nStay you to the first wicked levit.\\nAnd I proceet unloke for your caps! Frink, I say do.\\n\\nPOLIXENER:\\nNo, for.\\n\\nFRIAR LAURENCE:\\nBut sitws thee, was follow you:\\nWed us no fire done, sir,\\nTry Jup, was ever they but were true aught;\\nAnd put not this fea. away of thee!\\nFoul nothing can command, and had sacr me\\nput up my mouth that; by your enemies, what hath above way.\\n\\nDUKE VINCENTIO:\\nNo, mighty lord, they find looks so,\\nTo dispat a heart. And proud feal,\\nWhich we will rather to him; 'twere's gone.\\n\\nQUEEN ELIZABETH:\\nPray you, couch-her, they say him.\\nThou hadst thou look ot sleep.\\n\\nAfficer:\\nThat thou art diskonous and looks on feot.\\n\\nYORK:\\nThere's now? were he not sp\"\n",
            " b\"ROMEO:\\nThou mother!\\n\\nBIONDELLUS:\\nWe have not to a place o' the people.\\n\\nCORIOLANUS:\\nAlack, as my gratest friend: sith brother forbid\\nWit blush their likeness' horrirghanders.\\n\\nVALERIA:\\nHeaven, we have fin's witners and Bolingbroke,\\nWe use to't and fear ox, mother's love,\\nTo help upon thee. Close up in the nods of you?\\n\\nPAULINA:\\nI say; a broken allow. See, it.\\n\\nEBBAPTOR:\\nCould not to Ladut Bohemia you. For how Leaven she deny bount\\nTo be the action groans, true thousand and his brother\\nFor Edward'st thou that Edward be coly\\nThe devoued to be the sovereign. It is not know\\nTo sleep the boint of this armour of\\nYork, fear me to it. Sweet Clarence!\\n\\nSTANLEY:\\nPeter.\\n\\nBUCKINGHAM:\\nYet quickly from those things not hear?\\n\\nHERMIONE:\\nRight, you may is her, past with the tender fight it grow now\\nIf I put: go tiver, ours!\\n\\nFLORIZEL:\\nHow should be your grace to the lebtituth of\\nHis son Harbonardly unto Forse,\\nBeing Capulets Juliet. Now, Jume's My:\\nAfter Murgerina,\\nHe that is but tombsy fortune as you thus;\"\n",
            " b\"ROMEO:\\nThe kinsman is an oldect and thy look'd word\\nTo yield. Then never give no lissly?\\n\\nBooth:\\nThe god inconstant Bianca,\\nThat he so us wish. Peruse humour\\nTo break his dumbs plucker'd frowning down.\\nHis daughter: speak, I tell you, tell two sits him.\\n\\nLADY CAPULET:\\nWhither to lamen and wantle me! I hope before\\nGair your desire take your chape, that you have beboke upon him.\\n\\nDUKE VINCENTIO:\\nBe patient, like a grave? rench a granded, nire.\\nRatsle Warwick, spur for thy nabusy,\\nSuch as he lies in pying frothing; I hope for't.\\n\\nESCALUS:\\nGood of ceremonious sentence,\\nUn mouths of his delicatest--\\n\\nKING RICHARD II:\\nRepulant Warwick's sour: the drunk me?\\n\\nFursh:\\nLook you, deliver, nor any glory and seized\\nConciving to him, 'Romeo, AUREbBY come, young and favour,\\nI know you, sir.\\n\\nABTHANIO:\\nOld Aloun, will you go along.\\n\\nMARIANA:\\nMy dreams pletamicl, sir, as you.\\n\\nMARCIUS:\\nSo doubt it, I know it not to be another.\\nThe lark mad made you sleep even too, I am doth look'd away;\\nAnd lies upon the less\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.194274187088013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ekspor Model Generator"
      ],
      "metadata": {
        "id": "Rzu1x-E0FYV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2__083zDFVTg",
        "outputId": "f2fc54bf-05e1-438f-fbc3-a6912e24c6df"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7867671f3370>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbrSKFpxFaKU",
        "outputId": "e9552a6a-3df0-4356-cfb8-8cdf723027bb"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Should say York!\n",
            "How they do, fain, beceeter'd strangs, and firstice on the earth,\n",
            "I naved shed jud\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mtSA7dDxFdKH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}